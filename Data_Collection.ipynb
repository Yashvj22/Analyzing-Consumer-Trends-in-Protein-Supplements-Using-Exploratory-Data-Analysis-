{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb035fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541863df",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:106.0) Gecko/20100101Firefox/106.0',\n",
    "'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp./;q= 0.8',\n",
    "'Accept-Language': 'en-US,en;q=0.5',\n",
    "#'Accept-Encoding': 'gzip, deflate, br',\n",
    "'DNT': '1',\n",
    "'Connection': 'keep-alive',\n",
    "'Upgrade-Insecure-Requests': '1',\n",
    "'Sec-Fetch-Dest': 'document',\n",
    "'Sec-Fetch-Mode': 'navigate',\n",
    "'Sec-Fetch-Site': 'none',\n",
    "'Sec-Fetch-User': '?1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "54d7c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.flipkart.com/search?q=whey+protein+&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=1\"\n",
    "page = requests.get(url, headers= headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6a18ec93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "95c02a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a8ba5",
   "metadata": {},
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "321949a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "product_links = []\n",
    "All_Quantity = []\n",
    "All_Flavour = []\n",
    "Price = []\n",
    "Rating = []\n",
    "Total_rating_reviews = []\n",
    "Seller = []\n",
    "Seller_rating = []\n",
    "Brand = []\n",
    "Serving_Size = []\n",
    "Flavor = []\n",
    "Container_Type = []\n",
    "\n",
    "Net_Quantity = []\n",
    "scoops = []\n",
    "\n",
    "\n",
    "## First we will scrap all product links from Total Pages\n",
    "\n",
    "for i in range(1, 26):  \n",
    "    url = f\"https://www.flipkart.com/search?q=whey+protein+&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page={i}\"\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    ## Product_Links\n",
    "    for div in soup.find_all('div', {'class': 'slAVV4'}): \n",
    "        for link in div.find_all('a', href=True):  \n",
    "            href = link.get('href')\n",
    "            if href:  \n",
    "                full_url = 'https://www.flipkart.com' + href\n",
    "\n",
    "                ## if url not in list then only append To reduce problem of multiple urls\n",
    "                if full_url not in product_links:\n",
    "                    product_links.append(full_url)\n",
    "\n",
    "                    \n",
    "# Scrape data from individual products\n",
    "for product_url in product_links:\n",
    "    try:\n",
    "        res = requests.get(product_url, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        ## Quantity -> List Index 0\n",
    "        try:\n",
    "            ul_element = soup.find_all('ul', class_='hSEbzK')[0]\n",
    "            quantities = []\n",
    "            for li in ul_element.find_all('li'):\n",
    "                text = li.text.strip()\n",
    "                if len(text) % 2 == 0 and text[:len(text) // 2] == text[len(text) // 2:]:\n",
    "                    text = text[:len(text) // 2]\n",
    "                if text not in quantities:\n",
    "                    quantities.append(text)\n",
    "            All_Quantity.append(quantities)\n",
    "        except:\n",
    "            All_Quantity.append(None)\n",
    "\n",
    "        ## Flavour -> List Index 1\n",
    "        try:\n",
    "            ul_element = soup.find_all('ul', class_='hSEbzK')[1]\n",
    "            flavors = []\n",
    "            for li in ul_element.find_all('li'):\n",
    "                text = li.text\n",
    "                if (text[:len(text) // 2]).title() not in flavors:\n",
    "                    flavors.append((text[:len(text) // 2]).title())\n",
    "            All_Flavour.append(flavors)\n",
    "        except:\n",
    "            All_Flavour.append(None)\n",
    "\n",
    "        # Price\n",
    "        try:\n",
    "            price = soup.find_all('div', class_='Nx9bqj CxhGGd')[0]\n",
    "            Price.append(price.text.strip())\n",
    "        except:\n",
    "            Price.append(None)\n",
    "\n",
    "        # Rating\n",
    "        try:\n",
    "            rating = soup.find_all('div', class_='XQDdHH')[0]\n",
    "            Rating.append(rating.text.strip())\n",
    "        except:\n",
    "            Rating.append(None)\n",
    "\n",
    "        # Total Ratings and Reviews\n",
    "        try:\n",
    "            total_reviews = soup.find_all('span', class_='Wphh3N')[0]\n",
    "            Total_rating_reviews.append(total_reviews.text.strip())\n",
    "        except:\n",
    "            Total_rating_reviews.append(None)\n",
    "\n",
    "        # Seller Name\n",
    "        try:\n",
    "            seller = soup.find_all('div', class_='yeLeBC')[0]\n",
    "            Seller.append(seller.text.strip())\n",
    "        except:\n",
    "            Seller.append(None)\n",
    "\n",
    "        # Seller Ratings\n",
    "        try:\n",
    "            seller_rating = soup.find_all('div', class_='XQDdHH uuhqql')[0]\n",
    "            Seller_rating.append(seller_rating.text.strip())\n",
    "        except:\n",
    "            Seller_rating.append(None)\n",
    "\n",
    "        # Table Info\n",
    "        try:\n",
    "            info = []\n",
    "            values = []\n",
    "            length = len(soup.find_all('td', class_='+fFi1w col col-3-12'))\n",
    "            for i in range(length):\n",
    "                info.append(soup.find_all('td', class_='+fFi1w col col-3-12')[i].text.strip())\n",
    "\n",
    "            length = len(soup.find_all('td', class_='Izz52n col col-9-12'))\n",
    "            for i in range(length):\n",
    "                values.append(soup.find_all('td', class_='Izz52n col col-9-12')[i].text.strip())\n",
    "\n",
    "            dic = dict(zip(info, values))\n",
    "            Brand.append(dic.get('Brand', None))\n",
    "            Serving_Size.append(dic.get('Serving Size', None))\n",
    "            Flavor.append(dic.get('Flavor', None))\n",
    "            Container_Type.append(dic.get('Container Type', None))\n",
    "            Net_Quantity.append(dic.get('Net Quantity', None))\n",
    "            scoops.append(dic.get('Number of Scoops per Container', None))\n",
    "        except:\n",
    "            Brand.append(None)\n",
    "            Serving_Size.append(None)\n",
    "            Flavor.append(None)\n",
    "            Container_Type.append(None)\n",
    "            Net_Quantity.append(None)\n",
    "            scoops.append(None)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product: {product_url}, Error: {e}\")\n",
    "        All_Quantity.append(None)\n",
    "        All_Flavour.append(None)\n",
    "        Price.append(None)\n",
    "        Rating.append(None)\n",
    "        Total_rating_reviews.append(None)\n",
    "        Seller.append(None)\n",
    "        Seller_rating.append(None)\n",
    "        Brand.append(None)\n",
    "        Serving_Size.append(None)\n",
    "        Flavor.append(None)\n",
    "        Container_Type.append(None)\n",
    "        Net_Quantity.append(None)\n",
    "        scoops.append(None)\n",
    "\n",
    "print(len(product_links))\n",
    "print(len(All_Quantity))\n",
    "print(len(All_Flavour))\n",
    "print(len(Price))\n",
    "print(len(Rating))\n",
    "print(len(Total_rating_reviews))\n",
    "print(len(Seller))\n",
    "print(len(Seller_rating))\n",
    "print(len(Brand))\n",
    "print(len(Serving_Size))\n",
    "print(len(Flavor))\n",
    "print(len(Container_Type))\n",
    "print(len(Net_Quantity))\n",
    "print(len(scoops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b22f4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"All_Quantity\":All_Quantity,\"All_Flavour\":All_Flavour,\n",
    "        \"Price\":Price,\"Rating\":Rating,\"Total_rating_reviews\":Total_rating_reviews, \n",
    "        \"Seller\":Seller, \"Seller_rating\":Seller_rating, \"Brand\":Brand, \n",
    "        \"Serving_Size\":Serving_Size, \"Flavor\":Flavor, \"Container_Type\":Container_Type, \n",
    "        \"Net_Quantity\":Net_Quantity, \"scoops\":scoops, \"product_links\":product_links}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc856d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cf808ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the Data \n",
    "\n",
    "protein_df.to_csv(r\"C:\\Users\\acer\\Desktop\\Data Science Innomatics\\EDA\\Untitled Folder\\Protein.csv\",index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
